{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we will build a model to automatically classify tweet text into disaster-related or not disaster-related categories. This can help identify tweets discussing real-world disasters and expedite relief efforts.\n\nThe dataset comes from a Kaggle competition and contains ~10,000 tweets labeled as positive (relevant to disasters) or negative (not relevant).","metadata":{}},{"cell_type":"markdown","source":"### Imports and Settings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:18.690232Z","iopub.execute_input":"2023-09-18T05:41:18.690697Z","iopub.status.idle":"2023-09-18T05:41:18.698765Z","shell.execute_reply.started":"2023-09-18T05:41:18.690663Z","shell.execute_reply":"2023-09-18T05:41:18.697568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport subprocess\n\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:18.705607Z","iopub.execute_input":"2023-09-18T05:41:18.706731Z","iopub.status.idle":"2023-09-18T05:41:18.744003Z","shell.execute_reply.started":"2023-09-18T05:41:18.706694Z","shell.execute_reply":"2023-09-18T05:41:18.743026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nThe training data has 7613 labeled samples. Let's inspect some samples from each class.\n","metadata":{}},{"cell_type":"code","source":"tweets = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n\nprint(tweets['text'][tweets['target']==0].sample(5))\nprint(tweets['text'][tweets['target']==1].sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:18.745933Z","iopub.execute_input":"2023-09-18T05:41:18.747058Z","iopub.status.idle":"2023-09-18T05:41:18.784641Z","shell.execute_reply.started":"2023-09-18T05:41:18.747021Z","shell.execute_reply":"2023-09-18T05:41:18.783538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe use of abbreviations, hashtags, emojis typical of tweet language. Both classes discuss related topics like flooding and damage.\n\nLet's visualize the class distribution:","metadata":{}},{"cell_type":"code","source":"plt.bar([0,1], tweets['target'].value_counts())\nplt.xticks([0,1], ['Negative', 'Positive'])\nplt.title('Class Distribution')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:18.786626Z","iopub.execute_input":"2023-09-18T05:41:18.787320Z","iopub.status.idle":"2023-09-18T05:41:19.017949Z","shell.execute_reply.started":"2023-09-18T05:41:18.787282Z","shell.execute_reply":"2023-09-18T05:41:19.016990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows we have an imbalanced dataset with many more negative samples. We should consider techniques like oversampling to handle the class imbalance.\n\nLet's also look at the tweet length distribution:","metadata":{}},{"cell_type":"code","source":"tweets['text'].apply(len).hist(bins=30)\nplt.title('Tweet Length Distribution')\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:19.019374Z","iopub.execute_input":"2023-09-18T05:41:19.020226Z","iopub.status.idle":"2023-09-18T05:41:19.363624Z","shell.execute_reply.started":"2023-09-18T05:41:19.020191Z","shell.execute_reply":"2023-09-18T05:41:19.362424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows most tweets are short but still have content, less than 100 characters but more than 40.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing\n\nTo prepare the text for modeling, we will:\n- Normalize all characters to lowercase\n- Remove URLs, usernames, hashtags\n- Remove punctuation\n- Lemmatize text\n- Remove stopwords","metadata":{}},{"cell_type":"code","source":"stopwords = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', '', text)\n    text = text.replace('@', '').replace('#', '')\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n    return \" \".join(text)\n    \ntweets['text'] = tweets['text'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:19.366448Z","iopub.execute_input":"2023-09-18T05:41:19.367227Z","iopub.status.idle":"2023-09-18T05:41:19.927586Z","shell.execute_reply.started":"2023-09-18T05:41:19.367193Z","shell.execute_reply":"2023-09-18T05:41:19.926248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Model Building\n\nWe will split the data 80-20 into training and validation sets. \n\nThe text features will be encoded into TF-IDF vectors.\n\nA logistic regression classifier will be trained on the TF-IDF representations.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(tweets['text'], tweets['target'], test_size=0.2, random_state=42)\n\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train) \nX_valid = vectorizer.transform(X_valid)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:19.929027Z","iopub.execute_input":"2023-09-18T05:41:19.929431Z","iopub.status.idle":"2023-09-18T05:41:20.399913Z","shell.execute_reply.started":"2023-09-18T05:41:19.929393Z","shell.execute_reply":"2023-09-18T05:41:20.398487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nWe get ~80% validation accuracy with the logistic regression classifier. The classification report shows decent F1 scores for both classes.","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(X_valid)\n\nprint(accuracy_score(y_valid, predictions))\nprint(classification_report(y_valid, predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:20.401588Z","iopub.execute_input":"2023-09-18T05:41:20.401941Z","iopub.status.idle":"2023-09-18T05:41:20.450840Z","shell.execute_reply.started":"2023-09-18T05:41:20.401908Z","shell.execute_reply":"2023-09-18T05:41:20.449569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The simple NPL classifier does a decent job. Let's try some better techniques.","metadata":{}},{"cell_type":"markdown","source":"Let's oversample the minority positive class:","metadata":{}},{"cell_type":"code","source":"ros = RandomOverSampler(random_state=42)\nX_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)\n\nvectorizer = TfidfVectorizer()\nmodel = LogisticRegression()\nmodel.fit(X_train_ros, y_train_ros)\n\npredictions = model.predict(X_valid)\n\nprint(accuracy_score(y_valid, predictions))\nprint(classification_report(y_valid, predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:20.453000Z","iopub.execute_input":"2023-09-18T05:41:20.453781Z","iopub.status.idle":"2023-09-18T05:41:20.854010Z","shell.execute_reply.started":"2023-09-18T05:41:20.453742Z","shell.execute_reply":"2023-09-18T05:41:20.852872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also try other classifiers like Naive Bayes and SVM:","metadata":{}},{"cell_type":"code","source":"models = [\n    LogisticRegression(),\n    MultinomialNB(),\n    SVC()\n]\n\nfor model in models:\n    model.fit(X_train_ros, y_train_ros)\n    preds = model.predict(X_valid)\n    print(model)\n    print(accuracy_score(y_valid, preds))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:20.856022Z","iopub.execute_input":"2023-09-18T05:41:20.856904Z","iopub.status.idle":"2023-09-18T05:41:29.379062Z","shell.execute_reply.started":"2023-09-18T05:41:20.856861Z","shell.execute_reply":"2023-09-18T05:41:29.378079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, we can use neural networks:","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid') \n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:29.382940Z","iopub.execute_input":"2023-09-18T05:41:29.383899Z","iopub.status.idle":"2023-09-18T05:41:29.402277Z","shell.execute_reply.started":"2023-09-18T05:41:29.383862Z","shell.execute_reply":"2023-09-18T05:41:29.401115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_arr = X_train.toarray()\nmodel.fit(X_train_arr, y_train, epochs=5, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:29.403784Z","iopub.execute_input":"2023-09-18T05:41:29.404247Z","iopub.status.idle":"2023-09-18T05:41:36.343244Z","shell.execute_reply.started":"2023-09-18T05:41:29.404213Z","shell.execute_reply":"2023-09-18T05:41:36.342005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_train_arr, y_train)\nprint(f\"Loss: {loss}\")\nprint(f\"Accuracy: {accuracy}\")\n\nX_valid_arr = X_valid.toarray()\nloss, accuracy = model.evaluate(X_valid_arr, y_valid)\nprint(f\"Loss: {loss}\")\nprint(f\"Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:36.345390Z","iopub.execute_input":"2023-09-18T05:41:36.345848Z","iopub.status.idle":"2023-09-18T05:41:39.133271Z","shell.execute_reply.started":"2023-09-18T05:41:36.345809Z","shell.execute_reply":"2023-09-18T05:41:39.131932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The neural network did good on training, but validation accuracy wasn't the best.\n\nLet's try a stronger model architecture.\n\nFirst we will add padding to the training data.","metadata":{}},{"cell_type":"code","source":"MAX_NUM_WORDS = 10000  \nMAX_SEQUENCE_LENGTH = 100  \n\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\ntokenizer.fit_on_texts(tweets['text'])\nsequences = tokenizer.texts_to_sequences(tweets['text'])\npadded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:39.135251Z","iopub.execute_input":"2023-09-18T05:41:39.136042Z","iopub.status.idle":"2023-09-18T05:41:39.535702Z","shell.execute_reply.started":"2023-09-18T05:41:39.135998Z","shell.execute_reply":"2023-09-18T05:41:39.534556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we will make a new train test data split with the padded data.","metadata":{}},{"cell_type":"code","source":"X = padded_sequences\ny = tweets['target'].values\n\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:39.540150Z","iopub.execute_input":"2023-09-18T05:41:39.540493Z","iopub.status.idle":"2023-09-18T05:41:39.554142Z","shell.execute_reply.started":"2023-09-18T05:41:39.540464Z","shell.execute_reply":"2023-09-18T05:41:39.552751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for the model architecture, which includes an embedding layer, dropout, and LSTM which can help based on the large sequence of twitter data.","metadata":{}},{"cell_type":"code","source":"embedding_dim = 64\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=MAX_NUM_WORDS, output_dim=embedding_dim, input_length=MAX_SEQUENCE_LENGTH),\n    tf.keras.layers.SpatialDropout1D(0.3),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001) , metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:39.556135Z","iopub.execute_input":"2023-09-18T05:41:39.558806Z","iopub.status.idle":"2023-09-18T05:41:40.669718Z","shell.execute_reply.started":"2023-09-18T05:41:39.558768Z","shell.execute_reply":"2023-09-18T05:41:40.668897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also include early stopping and learning rate reduction to get the best optimization possible.","metadata":{}},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\nlr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=1, min_lr=0.00001)\n\nhistory = model.fit(X_train, y_train, batch_size=128, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping, checkpoint, lr_schedule], verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:41:40.670785Z","iopub.execute_input":"2023-09-18T05:41:40.671155Z","iopub.status.idle":"2023-09-18T05:42:24.384691Z","shell.execute_reply.started":"2023-09-18T05:41:40.671120Z","shell.execute_reply":"2023-09-18T05:42:24.383607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\n    \nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:42:24.386327Z","iopub.execute_input":"2023-09-18T05:42:24.387546Z","iopub.status.idle":"2023-09-18T05:42:25.102355Z","shell.execute_reply.started":"2023-09-18T05:42:24.387480Z","shell.execute_reply":"2023-09-18T05:42:25.101259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {loss}\")\nprint(f\"Test Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:42:25.103976Z","iopub.execute_input":"2023-09-18T05:42:25.104602Z","iopub.status.idle":"2023-09-18T05:42:25.508042Z","shell.execute_reply.started":"2023-09-18T05:42:25.104560Z","shell.execute_reply":"2023-09-18T05:42:25.507204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"test_tweets = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_tweets['text'] = test_tweets['text'].apply(preprocess)\ntest_sequences = tokenizer.texts_to_sequences(test_tweets['text'])\ntest_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\npredictions = model.predict(test_padded)\nbinary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\nsubmission_df = pd.DataFrame({'id': test_tweets['id'], 'target': binary_predictions})\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T05:42:25.510031Z","iopub.execute_input":"2023-09-18T05:42:25.510903Z","iopub.status.idle":"2023-09-18T05:42:28.581709Z","shell.execute_reply.started":"2023-09-18T05:42:25.510864Z","shell.execute_reply":"2023-09-18T05:42:28.580308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nThroughout this notebook, we engaged in the entire lifecycle of a natural language processing (NLP) project tailored for tweet classification. Key takeaways from our study include:\n\n1. **Data Examination:** Upon initial examination, the tweets in our dataset were found to be replete with elements typical of microblogging platforms: hashtags, mentions, emojis, and abbreviations. The dataset was somewhat imbalanced, with a higher number of negative samples.\n\n2. **Data Preprocessing:** Essential preprocessing steps were undertaken, including text normalization, removal of URLs, usernames, and special characters, as well as lemmatization. Stopwords were also excluded to improve the quality of the dataset.\n\n3. **Model Exploration:** Several models were evaluated:\n    - A Logistic Regression model achieved an accuracy of approximately 80%.\n    - Oversampling the minority class helped address the data imbalance, but the validation accuracy reduced slightly.\n    - Multiple classifiers, including Naive Bayes, SVM, and Neural Networks, were explored. Their performances varied, with the neural network model performing remarkably well on the training data but less so on validation.\n    \n4. **Neural Network Expansion:** To enhance the neural network's performance, a more robust architecture was employed. By introducing embeddings, dropout layers, and bidirectional LSTM layers, the model's capability to discern patterns in the sequence data was improved. Additionally, early stopping and learning rate reduction were used as strategies to optimize the model's performance.\n\n5. **Evaluation:** The LSTM-based deep learning model demonstrated a test accuracy of approximately 80%, which is a respectable figure considering the complexities and nuances of tweet language.\n\n6. **Submission:** The final model was used to predict the labels of a test dataset. The results were compiled and are ready for submission.\n\nIn summary, this study underscores the importance of a systematic and iterative approach to NLP. While the initial models offered decent performance, the continuous refinement of architecture and preprocessing led to improved results. Future iterations could potentially explore more sophisticated architectures, ensemble methods, or even the inclusion of external datasets to further enhance performance.","metadata":{}}]}