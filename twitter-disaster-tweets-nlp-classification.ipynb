{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nIn this notebook, we will build a model to automatically classify tweet text into disaster-related or not disaster-related categories. This can help identify tweets discussing real-world disasters and expedite relief efforts.\n\nThe dataset comes from a Kaggle competition and contains ~10,000 tweets labeled as positive (relevant to disasters) or negative (not relevant).","metadata":{}},{"cell_type":"markdown","source":"### Imports and Settings","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport re\nimport string","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:55:06.786585Z","iopub.execute_input":"2023-09-18T03:55:06.787113Z","iopub.status.idle":"2023-09-18T03:55:09.157793Z","shell.execute_reply.started":"2023-09-18T03:55:06.787066Z","shell.execute_reply":"2023-09-18T03:55:09.156234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport subprocess\n\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:55:09.159797Z","iopub.execute_input":"2023-09-18T03:55:09.160388Z","iopub.status.idle":"2023-09-18T03:55:10.587050Z","shell.execute_reply.started":"2023-09-18T03:55:09.160352Z","shell.execute_reply":"2023-09-18T03:55:10.585308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nThe training data has 7613 labeled samples. Let's inspect some samples from each class.\n","metadata":{}},{"cell_type":"code","source":"tweets = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n\nprint(tweets['text'][tweets['target']==0].sample(5))\nprint(tweets['text'][tweets['target']==1].sample(5))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:55:10.589285Z","iopub.execute_input":"2023-09-18T03:55:10.590438Z","iopub.status.idle":"2023-09-18T03:55:10.668611Z","shell.execute_reply.started":"2023-09-18T03:55:10.590380Z","shell.execute_reply":"2023-09-18T03:55:10.667392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe use of abbreviations, hashtags, emojis typical of tweet language. Both classes discuss related topics like flooding and damage.\n\n## Data Preprocessing\n\nTo prepare the text for modeling, we will:\n- Normalize all characters to lowercase\n- Remove URLs, usernames, hashtags\n- Remove punctuation\n- Lemmatize text\n- Remove stopwords","metadata":{}},{"cell_type":"code","source":"stopwords = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+', '', text)\n    text = text.replace('@', '').replace('#', '')\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stopwords]\n    return \" \".join(text)\n    \ntweets['text'] = tweets['text'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:55:10.671200Z","iopub.execute_input":"2023-09-18T03:55:10.672367Z","iopub.status.idle":"2023-09-18T03:55:14.044342Z","shell.execute_reply.started":"2023-09-18T03:55:10.672298Z","shell.execute_reply":"2023-09-18T03:55:14.042953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Model Building\n\nWe will split the data 80-20 into training and validation sets. \n\nThe text features will be encoded into TF-IDF vectors.\n\nA logistic regression classifier will be trained on the TF-IDF representations.\n","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(tweets['text'], tweets['target'], test_size=0.2, random_state=42)\n\nvectorizer = TfidfVectorizer()\nX_train = vectorizer.fit_transform(X_train) \nX_valid = vectorizer.transform(X_valid)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:55:44.406968Z","iopub.execute_input":"2023-09-18T03:55:44.407599Z","iopub.status.idle":"2023-09-18T03:55:45.475292Z","shell.execute_reply.started":"2023-09-18T03:55:44.407550Z","shell.execute_reply":"2023-09-18T03:55:45.473481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nWe get ~80% validation accuracy with the logistic regression classifier. The classification report shows decent F1 scores for both classes.","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(X_valid)\n\nprint(accuracy_score(y_valid, predictions))\nprint(classification_report(y_valid, predictions))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T03:56:21.170583Z","iopub.execute_input":"2023-09-18T03:56:21.170969Z","iopub.status.idle":"2023-09-18T03:56:21.198885Z","shell.execute_reply.started":"2023-09-18T03:56:21.170939Z","shell.execute_reply":"2023-09-18T03:56:21.197467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, we built a simple NLP classifier to detect disaster-related tweets. The steps included:\n\n- Exploring the tweet dataset\n- Preprocessing the text data\n- Creating TF-IDF features\n- Fitting a logistic regression model\n- Evaluating on a held-out set\n\nSome ways to improve the model would be:\n- Using word embeddings instead of TF-IDF\n- Trying other classifiers like SVM, RNNs\n- Expanding the dataset size using augmentation\n- Ensembling multiple models\n\nThis provides a template to get started with identifying disaster tweets using NLP. The techniques can be extended to build a robust real-world system.","metadata":{}}]}